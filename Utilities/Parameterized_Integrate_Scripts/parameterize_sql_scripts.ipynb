{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import re\n",
    "\n",
    "import sqlparse\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import sqlalchemy\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_contents_file(file_path, encoding_scheme='cp1252', read_lines=False):\n",
    "    inputfile = open(file_path, 'r', encoding=encoding_scheme)\n",
    "    if read_lines:\n",
    "        return inputfile.readlines()\n",
    "    else:\n",
    "        return inputfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write file to local directory\n",
    "def write_file_local(path,file_data,is_list=False):\n",
    "    \n",
    "    with open(path, 'w') as file:\n",
    "        if is_list:\n",
    "            file_string = '\\n'.join(file_data)\n",
    "        else:\n",
    "            file_string = file_data\n",
    "        file.write(file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unique_index(database, table, user_name, password, host_name):\n",
    "    if table.endswith('_mv') :\n",
    "            table = table[:-3]\n",
    "            print(table)\n",
    "    if table.endswith('_old') :\n",
    "            table = table[:-4]\n",
    "            print(table)\n",
    "    td_engine = sqlalchemy.create_engine('teradatasql://' + host_name + '/?user=' + user_name + '&password=' + password + '&logmech=KRB5')\n",
    "    query = f\"SELECT columnname FROM dbc.indicesV where databasename = '{database}' and tablename = '{table}'   and uniqueflag = 'Y' order by columnposition\" \n",
    "    try:\n",
    "        results_df = pd.read_sql(query, td_engine)\n",
    "    except Exception as e1:\n",
    "        print(\"Exception occured when getting unique index\")            \n",
    "        pass\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_comment(line):\n",
    "    return line.strip().startswith(\"--\") or line.strip().startswith(\"/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_duplicate_checks(convertedsql_path, postdupcheckssql_path, user_name, password, host_name):\n",
    "\n",
    "    print(\"Duplicate Checks Generation has begun\") \n",
    "    \n",
    "    if os.path.exists(postdupcheckssql_path):\n",
    "        shutil.rmtree(postdupcheckssql_path, ignore_errors=True)\n",
    "    os.makedirs(postdupcheckssql_path)\n",
    "    print(\"Created folder : {}\".format(postdupcheckssql_path))\n",
    "\n",
    "    regexp_EOF = re.compile(r\"\\bEOF\\b\",re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    file_list = [filename for filename in os.listdir(f\"{convertedsql_path}\") if filename.endswith(\".sql\")]\n",
    "    for filename in file_list:\n",
    "        source_file_path = os.path.join(convertedsql_path, filename).lower()\n",
    "        converted_sql = read_contents_file(source_file_path)\n",
    "\n",
    "        modified_stmnt = \"DECLARE DUP_COUNT INT64;\\n\"\n",
    "        statements = converted_sql.split(';')\n",
    "\n",
    "        in_comment_block = False\n",
    "\n",
    "        for stmnt in statements:\n",
    "            if stmnt.strip():\n",
    "                stmnt =  stmnt.strip()  + ';\\n'\n",
    "\n",
    "            if 'TABLE=' in stmnt.upper().strip() \\\n",
    "                or 'JOB=' in stmnt.upper().strip() \\\n",
    "                or stmnt.strip().startswith('#') \\\n",
    "                or stmnt.strip().startswith('locking') \\\n",
    "                or not regexp_EOF.search(stmnt) is None \\\n",
    "                or 'FOR SESSION' in stmnt.upper().strip() :\n",
    "                    stmnt = \"--\" + stmnt.strip() + ';\\n'\n",
    "\n",
    "            if stmnt.startswith(\"/*\"):\n",
    "                in_comment_block = True\n",
    "                loc_comnt_begin = stmnt.find(\"/*\")\n",
    "            else:\n",
    "                loc_comnt_begin = 0\n",
    "            if stmnt.find(\"*/\") != -1:\n",
    "                loc_comnt_end = stmnt.find(\"*/\")\n",
    "                if loc_comnt_end > loc_comnt_begin:\n",
    "                    in_comment_block = False\n",
    "\n",
    "            if not in_comment_block and not is_comment(stmnt):\n",
    "                if (\"INSERT \" in stmnt.upper() or \"MERGE \" in stmnt.upper()):\n",
    "                    stmnt_upper = stmnt.upper()\n",
    "                    tablename = stmnt_upper.split('INTO')[1].split()[0]\n",
    "                    tablenamesplits = tablename.split(\".\")\n",
    "                    if len(tablenamesplits) == 3 :\n",
    "                        tdtable = tablenamesplits[-1]\n",
    "                        tdschema = tablenamesplits[-2]\n",
    "                    elif len(tablenamesplits) == 2 :\n",
    "                        tdtable = tablenamesplits[1]\n",
    "                        tdschema = tablenamesplits[0]\n",
    "\n",
    "                    uniquecolumn_df = get_unique_index(tdschema, tdtable, user_name, password, host_name)\n",
    "\n",
    "                    if not uniquecolumn_df.empty:\n",
    "                        col = ''\n",
    "                        for column in uniquecolumn_df['ColumnName']:\n",
    "                            col = col + str(column).lower() + ','\n",
    "                        # print(\"Unique constraint Columns Found - \" + col)\n",
    "                        fullyqualifiedbqtablename = '`' + project_id + '`.' + tdschema+'.'+ tdtable\n",
    "                        fullyqualifiedbqtablename = fullyqualifiedbqtablename.lower()\n",
    "                        dup_query = \"SET DUP_COUNT = (\\nselect count(*)\\nfrom (\\nselect\\n\"+ col[:-1] +\"\\nfrom \"+ fullyqualifiedbqtablename + \" group by \" + col[:-1] + \"\\nhaving count(*) > 1\\n)\\n);\\nIF DUP_COUNT <> 0 THEN ROLLBACK TRANSACTION; RAISE USING MESSAGE = concat('Duplicates are not allowed in the table \" + fullyqualifiedbqtablename + \"'); ELSE COMMIT TRANSACTION; END IF;\\n\"\n",
    "                        modified_stmnt += \"BEGIN TRANSACTION;\\n\" + stmnt +  dup_query  \n",
    "                    else:\n",
    "                        modified_stmnt +=  stmnt  \n",
    "                else:\n",
    "                    modified_stmnt +=  stmnt \n",
    "            else:\n",
    "                modified_stmnt += '\\n' +  stmnt + '\\n'\n",
    "        \n",
    "        formattedsql = modified_stmnt.strip()\n",
    "        formattedsql = sqlparse.format(formattedsql, reindent=True, keyword_case='upper')\n",
    "        \n",
    "        copy_file_path = os.path.join(postdupcheckssql_path, filename).lower()\n",
    "        write_file_local(copy_file_path,formattedsql)\n",
    "\n",
    "    print(\"Duplicate Checks Generation is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_bqsqls(postdupcheckssql_path, postprocess_sql_path):\n",
    "\n",
    "    print(\"Post Processing has begun\") \n",
    "    \n",
    "    if os.path.exists(postprocess_sql_path):\n",
    "        shutil.rmtree(postprocess_sql_path, ignore_errors=True)\n",
    "    os.makedirs(postprocess_sql_path)\n",
    "    print(\"Created folder : {}\".format(postprocess_sql_path))\n",
    "\n",
    "    file_list = [filename for filename in os.listdir(f\"{postdupcheckssql_path}\") if filename.endswith(\".sql\")]\n",
    "    for filename in file_list:\n",
    "        source_file_path = os.path.join(postdupcheckssql_path, filename).lower()\n",
    "        converted_sql = read_contents_file(source_file_path)\n",
    "\n",
    "        for i in range(len(post_process_find_replace_list)):\n",
    "            converted_sql = converted_sql.replace(post_process_find_replace_list[i][\"search\"], post_process_find_replace_list[i]['replace'])\n",
    "\n",
    "        for i in range(len(post_process_regex_find_replace_list)):\n",
    "            regexp_pattern = re.compile(post_process_regex_find_replace_list[i][\"search\"], re.IGNORECASE)\n",
    "            regexp_repl_pattern = post_process_regex_find_replace_list[i][\"replace\"]\n",
    "            converted_sql = re.sub(regexp_pattern, regexp_repl_pattern, converted_sql)\n",
    "\n",
    "        formattedsql = converted_sql.strip()\n",
    "        formattedsql = sqlparse.format(formattedsql, reindent=True, keyword_case='upper')\n",
    "\n",
    "        for i in range(len(post_process_find_replace_list)):\n",
    "            formattedsql = formattedsql.replace(post_process_find_replace_list[i][\"search\"], post_process_find_replace_list[i]['replace'])  \n",
    "\n",
    "        for i in range(len(post_process_regex_find_replace_list)):\n",
    "            regexp_pattern = re.compile(post_process_regex_find_replace_list[i][\"search\"], re.IGNORECASE)\n",
    "            regexp_repl_pattern = post_process_regex_find_replace_list[i][\"replace\"]\n",
    "            formattedsql = re.sub(regexp_pattern, regexp_repl_pattern, formattedsql)\n",
    "\n",
    "        formattedsql = formattedsql.strip(' ;')\n",
    "        copy_file_path = os.path.join(postprocess_sql_path, filename).lower()\n",
    "        # print(copy_file_path)\n",
    "        write_file_local(copy_file_path,formattedsql)\n",
    "\n",
    "    print(\"Post Processing is completed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(postprocess_sql_path):\n",
    "    print(f\"executing SQL files in  {postprocess_sql_path}\" )\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    for file_name in os.listdir(postprocess_sql_path):\n",
    "        if file_name.endswith('.sql'):\n",
    "            sql_file_path = os.path.join(postprocess_sql_path, file_name)\n",
    "            sql_query = read_contents_file(sql_file_path)\n",
    "            for i in range(len(execution_time_find_replace_list)):\n",
    "                sql_query = sql_query.replace(execution_time_find_replace_list[i][\"search\"], execution_time_find_replace_list[i]['replace'])\n",
    "            try:         \n",
    "                client.query(sql_query, project=project_id, location='US').result()\n",
    "                logging.info(f\"SQL file {file_name} executed successfully.\")\n",
    "                print(f\"SQL file {file_name} executed successfully.\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error executing SQL file {file_name}: {e}\")\n",
    "                print(f\"Error executing SQL file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_execution_results(logfilename):\n",
    "    total_lines = 0\n",
    "    success_lines = 0\n",
    "    failure_lines = 0\n",
    "    missing_dbobjects = 0 \n",
    "    dup_data_issues = 0\n",
    "\n",
    "    try:\n",
    "        with open(logfilename, \"r\") as file:\n",
    "            print(\"=========Errors================\")\n",
    "            for line in file:\n",
    "                if \"SQL file\" in line:\n",
    "                    total_lines += 1\n",
    "                    if \"executed successfully\" in line:\n",
    "                        success_lines += 1\n",
    "                    elif \"Not found\" in line:\n",
    "                        missing_dbobjects  += 1    \n",
    "                        print(line.strip())                    \n",
    "                        failure_lines += 1\n",
    "                    elif \"Duplicates\" in line or \"at most one source row\" in line :\n",
    "                        dup_data_issues  += 1    \n",
    "                        print(line.strip())                    \n",
    "                        failure_lines += 1\n",
    "                    else:\n",
    "                        failure_lines += 1\n",
    "                        print(line.strip())\n",
    "\n",
    "        print(\"=========execution_results================\")\n",
    "        print(\"Total SQL files executed:\", total_lines)\n",
    "        print(\"SQL files executed successfully:\", success_lines)\n",
    "        print(\"SQL files failed with errors:\", failure_lines)\n",
    "        print(\" --SQL files with Missing db objects:\", missing_dbobjects)\n",
    "        print(\" --SQL files with Dup Data Issues:\", dup_data_issues)\n",
    "\n",
    "\n",
    "        logging.info(\"=========execution_results================\")\n",
    "        logging.info(f\"Total SQL files executed: {total_lines}\")\n",
    "        logging.info(f\"SQL files executed successfully: {success_lines}\")\n",
    "        logging.info(f\"SQL files failed with errors: {failure_lines}\")\n",
    "        logging.info(f\" --SQL files with Missing db objects: {missing_dbobjects}\")\n",
    "        logging.info(f\" --SQL files with Dup Data Issues: {dup_data_issues}\")\n",
    "                     \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{logfilename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productionize_sqls(postprocessqls, prodready_bqsqls):\n",
    "\n",
    "    if os.path.exists(prodready_bqsqls):\n",
    "        shutil.rmtree(prodready_bqsqls, ignore_errors=True)\n",
    "    os.makedirs(prodready_bqsqls)\n",
    "    print(\"Created folder : {}\".format(prodready_bqsqls))\n",
    "\n",
    "    df_job_source = pd.read_csv(\"config\\script_source_map.csv\", index_col=None)\n",
    "    \n",
    "    for file in os.listdir(f\"{postprocessqls}\"):\n",
    "        if file.endswith('.sql'):\n",
    "            source_file_path = os.path.join(postprocessqls, file)\n",
    "            post_process_sql = read_contents_file(source_file_path)\n",
    "            \n",
    "            filename = file.replace(\".sql\",\"\").lower()\n",
    "            \n",
    "            df_job_source_match = df_job_source[df_job_source[\"script_name\"]==filename]\n",
    "            if df_job_source_match.empty:\n",
    "                source_per_job_name = \"\"\n",
    "            else:\n",
    "                source_per_job_name = df_job_source_match[\"source\"].iloc[0]\n",
    "\n",
    "            for i in range(len(production_regex_find_replace_list)):\n",
    "                regexp_pattern = re.compile(production_regex_find_replace_list[i][\"search\"], re.IGNORECASE | re.DOTALL)\n",
    "                regexp_repl_pattern = production_regex_find_replace_list[i][\"replace\"]\n",
    "                post_process_sql = re.sub(regexp_pattern, regexp_repl_pattern, post_process_sql)\n",
    "\n",
    "            for i in range(len(production_parms_find_replace_list)):\n",
    "                post_process_sql = post_process_sql.replace(production_parms_find_replace_list[i][\"search\"], production_parms_find_replace_list[i][\"replace\"])  \n",
    "\n",
    "            formattedsql = post_process_sql.strip()\n",
    "            formattedtext = sqlparse.format(formattedsql, reindent=True, keyword_case='upper', strip_comments=False)\n",
    "            \n",
    "            os.makedirs(f\"{prodready_bqsqls}\\{source_per_job_name}\", exist_ok=True)\n",
    "            copy_file_path = os.path.join(prodready_bqsqls, source_per_job_name, file).lower()\n",
    "            write_file_local(copy_file_path, formattedtext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dags(prodready_bqsqls):\n",
    "    # print(prodready_bqsqls)\n",
    "    command = 'gsutil -m cp -r ' + prodready_bqsqls + '\\**\\*.sql gs://' + dag_bucket_path\n",
    "\n",
    "    try:\n",
    "        completed_process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(completed_process.stdout)\n",
    "        print(\"Uploaded Dags to {}\".format(dag_bucket_path))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = datetime.now()\n",
    "# run_time = (dt1).strftime('%Y%m%d_%H%M')\n",
    "run_time = \"20240909_0643\"\n",
    "\n",
    "with open('config/lob_config.json') as json_lob_config:\n",
    "    config = json.load(json_lob_config)\n",
    "\n",
    "lob = config['lob']\n",
    "lob_abbr = config['lob_abbr'] # lobname in BQMS Script\n",
    "\n",
    "lob_lower = lob.strip().lower()\n",
    "lob_upper = lob.strip().upper()\n",
    "lob_abbr_lower = lob_abbr.strip().lower()\n",
    "lob_abbr_upper = lob_abbr.strip().upper()\n",
    "\n",
    "log_folder = config['log_folder']\n",
    "log_path_folder = f\"{log_folder}\\{lob_abbr_lower}\"\n",
    "\n",
    "if not os.path.exists(log_path_folder):\n",
    "    os.makedirs(log_path_folder)\n",
    "\n",
    "logfilename = lob_abbr_lower + '_param_execution_' + run_time + '.log'\n",
    "logfilenamepath = os.path.join(log_path_folder, logfilename)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(filename=logfilenamepath, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "conversion_target_folder = config['conversion_target_folder']\n",
    "conversion_target_path_folder = f\"{conversion_target_folder}\\{lob_abbr_lower}\"\n",
    "\n",
    "post_dupchecks_bqsqls  = config['post_dupchecks_bqsqls']\n",
    "post_dupchecks_bqsqls_path = f\"{post_dupchecks_bqsqls}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "postprocessed_bqsqlsfiles = config['postprocessqls']\n",
    "postprocessed_bqsqlsfiles_path = f\"{postprocessed_bqsqlsfiles}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "prodready_bqsqls = config['prodready_bqsqls']\n",
    "prodready_bqsqls_path = f\"{prodready_bqsqls}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "project_id = config['project_id']\n",
    "fns_project_id = project_id\n",
    "fns_dataset = config['fns_dataset']\n",
    "dag_bucket_path = config['dag_bucket_path']\n",
    "\n",
    "user_name = config['user_name']\n",
    "password = config['password']\n",
    "host_name = config['host_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # CR Replacements\n",
    "\n",
    "    # post_process_find_replace_list = [\n",
    "    #     {'search':'!=','replace':'<>'},\n",
    "    #     {'search':\"','AS\",'replace':\"',' AS\"},\n",
    "    #     {'search':'syslib.length','replace':'length'},\n",
    "    #     {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-ops`.bqutil_fns.isnumeric'},\n",
    "    # ]\n",
    "\n",
    "    # post_process_regex_find_replace_list = [\n",
    "    #     {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    #     'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    #     {'search':r\"td_sysfnlib\\.to_number\\((.*?)\\)\", 'replace':r\"safe_cast(\\1 as numeric) \"},\n",
    "    #     {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-ops`.bqutil_fns.\"}\n",
    "    # ]\n",
    "\n",
    "    # execution_time_find_replace_list = [\n",
    "    # #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    # ]\n",
    "\n",
    "    # production_parms_find_replace_list = [\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_staging.','replace':'{{ params.param_cr_stage_dataset_name }}.'},        \n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr.','replace':'{{ params.param_cr_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_base_views.','replace':'{{ params.param_cr_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.bqutil_fns.','replace':'{{ params.param_cr_bqutil_fns_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.auth_base_views.','replace':'{{ params.param_cr_auth_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_views.','replace':'{{ params.param_cr_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edw_pub_views.clinical_facility','replace':'{{ params.param_cr_auth_base_views_dataset_name }}.clinical_facility'}\n",
    "    # ]\n",
    "\n",
    "    # production_regex_find_replace_list = [\n",
    "    #     {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)\\s*=\\s*current_date\\('US\\/Central'\\)\\s*\", \n",
    "    #     'replace':r\"\\1 >= tableload_start_time - INTERVAL 1 MINUTE \"},\n",
    "    #     {'search':r\"([a-z|0-9|_]+.dw_last_update_date_time)\\s*[<>=]\\s*\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    #     'replace':r\"\\1 = current_date('US/Central') \"},\n",
    "    #     {'search':r\"\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    #     'replace':r\" tableload_start_time - INTERVAL 1 MINUTE \"}\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IM Replacements\n",
    "\n",
    "post_process_find_replace_list = [\n",
    "    {'search':'!=','replace':'<>'},\n",
    "    {'search':\"','AS\",'replace':\"',' AS\"},\n",
    "    {'search':'syslib.length','replace':'length'},\n",
    "    {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-pub`.bqutil_fns.isnumeric'},\n",
    "]\n",
    "\n",
    "post_process_regex_find_replace_list = [\n",
    "    {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    {'search':r\"td_sysfnlib\\.to_number\\((.*?)\\)\", 'replace':r\"safe_cast(\\1 as numeric) \"},\n",
    "    {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-pub`.bqutil_fns.\"},\n",
    "    {'search':r\"edw_pub_views\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views\"},\n",
    "    {'search':r\"edwcdm_base_views\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views\"},\n",
    "    {'search':r\"edwdw_base_views\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views\"}\n",
    "]\n",
    "\n",
    "execution_time_find_replace_list = [\n",
    "#    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "]\n",
    "\n",
    "production_parms_find_replace_list = [\n",
    "        {'search':'edwim_staging.','replace':'{{ params.param_im_stage_dataset_name }}.'},        \n",
    "        {'search':'edwim.','replace':'{{ params.param_im_core_dataset_name }}.'},\n",
    "        {'search':'edwim_base_views.','replace':'{{ params.param_im_base_views_dataset_name }}.'},\n",
    "        {'search':'edwim_views.','replace':'{{ params.param_im_views_dataset_name }}.'},\n",
    "        {'search':'auth_base_views.','replace':'{{ params.param_im_auth_base_views_dataset_name }}.'},\n",
    "        {'search':'`hca-hin-dev-cur-pub`.bqutil_fns.','replace':'{{ params.param_im_bqutil_fns_dataset_name }}.'}\n",
    "]\n",
    "\n",
    "production_regex_find_replace_list = [\n",
    "    {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)\\s*=\\s*current_date\\('US\\/Central'\\)\\s*\", \n",
    "    'replace':r\"\\1 >= tableload_start_time - INTERVAL 1 MINUTE \"},\n",
    "    {'search':r\"([a-z|0-9|_]+.dw_last_update_date_time)\\s*[<>=]\\s*\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    'replace':r\"\\1 = current_date('US/Central') \"},\n",
    "    {'search':r\"\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    'replace':r\" tableload_start_time - INTERVAL 1 MINUTE \"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the DDLs for the tables from teradata \n",
    "def parameterize_sql_scripts():\n",
    "\n",
    "    # Generate Duplicate Checks\n",
    "    gen_duplicate_checks(conversion_target_path_folder, post_dupchecks_bqsqls_path, user_name, password, host_name)\n",
    "\n",
    "    # Postprocess SQL Files\n",
    "    postprocess_bqsqls(post_dupchecks_bqsqls_path, postprocessed_bqsqlsfiles_path)\n",
    "    \n",
    "    # Execute Postprocessed SQL FIles\n",
    "    execute_sql(postprocessed_bqsqlsfiles_path)\n",
    "\n",
    "    # Print Execution Results\n",
    "    print_execution_results(logfilenamepath)\n",
    "\n",
    "    # Parameterize the sqls\n",
    "    productionize_sqls(postprocessed_bqsqlsfiles_path, prodready_bqsqls_path)\n",
    "\n",
    "    # Upload the Templated SQLs to DAG\n",
    "    # upload_to_dags(output_path_folder)\n",
    "    \n",
    "    dt2 = datetime.now()\n",
    "    print(dt2-dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin of Processing\")\n",
    "\n",
    "parameterize_sql_scripts()\n",
    "\n",
    "print(\"End of Processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
