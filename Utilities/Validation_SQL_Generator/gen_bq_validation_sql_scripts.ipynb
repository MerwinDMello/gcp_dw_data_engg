{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "import paramiko\n",
    "\n",
    "import re\n",
    "import shlex\n",
    "import glob\n",
    "\n",
    "import sqlparse\n",
    "\n",
    "from jinja2 import Environment, FileSystemLoader\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from google.cloud import storage\n",
    "from google.cloud import bigquery_migration_v2\n",
    "from google.cloud import bigquery\n",
    "\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_contents_file(file_path, encoding_scheme='cp1252', read_lines=False):\n",
    "    inputfile = open(file_path, 'r', encoding=encoding_scheme)\n",
    "    if read_lines:\n",
    "        return inputfile.readlines()\n",
    "    else:\n",
    "        return inputfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write file to local directory\n",
    "def write_file_local(path,file_data,is_list=False):\n",
    "    \n",
    "    with open(path, 'w') as file:\n",
    "        if is_list:\n",
    "            file_string = '\\n'.join(file_data)\n",
    "        else:\n",
    "            file_string = file_data\n",
    "        file.write(file_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sftp_download_bteqs(hostname, username, password, remotedirectory, localdirectory, srcsysname, file_prefix, add_source_dir_prefix):\n",
    "\n",
    "    try:\n",
    "        ssh_client=paramiko.SSHClient()\n",
    "        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        ssh_client.connect(hostname=hostname,username=username,password=password)\n",
    "        sftp_client=ssh_client.open_sftp()\n",
    "\n",
    "        file_list = sftp_client.listdir(path=remotedirectory)\n",
    "        for remote_file in file_list:\n",
    "            if remote_file.endswith('.sql') or remote_file.endswith('.prm') or remote_file.endswith('.sh'):\n",
    "                if remote_file.startswith(file_prefix) and file_prefix.strip() != \"\" or file_prefix.strip() == \"\":\n",
    "                    file_name = os.path.basename(remote_file)\n",
    "                    \n",
    "                    # print(\"Downloading {}\".format(file_name))\n",
    "                    if add_source_dir_prefix == \"Yes\":\n",
    "                        local_file_path = os.path.join(localdirectory, srcsysname + '_' + file_name)\n",
    "                    else:\n",
    "                        local_file_path = os.path.join(localdirectory, file_name)\n",
    "                    remote_file_path = os.path.join(remotedirectory, file_name)\n",
    "                    try: \n",
    "                        sftp_client.get(remote_file_path, local_file_path)\n",
    "                    except:\n",
    "                        print(\"Error downloading file {}\".format(remote_file_path))        \n",
    "\n",
    "    except paramiko.AuthenticationException:\n",
    "        print(\"Authentication failed. Please check your credentials.\")\n",
    "    except paramiko.SSHException:\n",
    "        print(\"An error occurred while establishing an SSH connection.\")\n",
    "    except Exception as e:\n",
    "        print(\"Error: {}\".format(str(e)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sftp_download_bteqs_multiple_sources(hostname, username, password, unix_etl_folder, unix_lob, localdirectory, lob_sub_folders, file_prefix=\"\", add_source_dir_prefix=\"No\", parmfolder_before_lob_subfolder=\"No\"):\n",
    "\n",
    "    if os.path.exists(localdirectory):\n",
    "        shutil.rmtree(localdirectory, ignore_errors=True)\n",
    "    os.makedirs(localdirectory)\n",
    "    print(\"Created folder : {}\".format(localdirectory))\n",
    "\n",
    "    lobbasefolder = f\"{unix_etl_folder}{unix_lob}\"\n",
    "    print(lobbasefolder)\n",
    "    for srcfolder in lob_sub_folders:\n",
    "        if srcfolder == 'ParmFiles':\n",
    "            remotedirectory = f\"{lobbasefolder}/{srcfolder}/\"\n",
    "            sftp_download_bteqs(hostname, username, password, remotedirectory,  localdirectory, unix_lob, file_prefix, add_source_dir_prefix)\n",
    "        elif parmfolder_before_lob_subfolder == 'Yes':\n",
    "            remotedirectory = f\"{lobbasefolder}/ParmFiles/{srcfolder}/\"\n",
    "            sftp_download_bteqs(hostname, username, password, remotedirectory,  localdirectory, srcfolder, file_prefix, add_source_dir_prefix)\n",
    "        else:\n",
    "            remotedirectory = f\"{lobbasefolder}/{srcfolder}/ParmFiles/\"\n",
    "            sftp_download_bteqs(hostname, username, password, remotedirectory,  localdirectory, srcfolder, file_prefix, add_source_dir_prefix)\n",
    "\n",
    "    print(\"Copied Shell Files\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_variables_and_gen_td_sql_files(parmfile_path, conversion_source_path_folder, sub_folder_list):\n",
    "    try:\n",
    "        print(\"Read Shell Files\") \n",
    "        \n",
    "        control_rec_list = []\n",
    "\n",
    "        regexp_edw = re.compile(r\"(edw)\",re.IGNORECASE)\n",
    "        regexp_core = re.compile(r\"(\"+'|'.join(core_views_list)+\")\\.\",re.IGNORECASE)\n",
    "        regexp_staging = re.compile(r\"(\"+'|'.join(source_dataset_list)+\")\\.\",re.IGNORECASE)\n",
    "        regexp_select = re.compile(r\"select .*as source_string\",re.IGNORECASE)\n",
    "        \n",
    "        for folder in sub_folder_list:\n",
    "            sub_folder = f\"{conversion_source_path_folder}\\{folder}\"\n",
    "            if os.path.exists(sub_folder):\n",
    "                shutil.rmtree(sub_folder, ignore_errors=True)\n",
    "            os.makedirs(sub_folder)\n",
    "            print(\"Created folder : {}\".format(sub_folder))\n",
    "\n",
    "        ########################## CAPTURE EXPECTED & ACTUAL VALIDATION SQLs ##########################\n",
    "\n",
    "        for filename in os.listdir(parmfile_path):\n",
    "\n",
    "            file_relative_path = os.path.join(parmfile_path, filename)\n",
    "            validation_sql_contents = read_contents_file(file_relative_path)\n",
    "            \n",
    "            filename = filename.lower().split('.')[0]\n",
    "            if filename.endswith(\"_prm\"):\n",
    "                filename = filename[:-len(\"_prm\")]\n",
    "            \n",
    "            #Remove Empty Lines\n",
    "            validation_sql_contents = os.linesep.join([s for s in validation_sql_contents.splitlines() if s])\n",
    "\n",
    "            #Remove Export keywords as they won't be parsed by shlex\n",
    "            processed = validation_sql_contents.replace('export ', '')\n",
    "            \n",
    "            shell_vars = dict()\n",
    "\n",
    "            # Parse Shell Script to capture coding lines and ignore comments\n",
    "            for line in shlex.split(processed, comments=True):\n",
    "                # Capture Variable & Value\n",
    "                var, _, var_value = line.partition('=')\n",
    "\n",
    "                # Remove WhiteSpace and then remove additional space characters, lowercase the data\n",
    "                # shell_vars[var] = re.sub(\" +\", \" \",re.sub(\"[\\r]|[\\n]|[\\t]\",\" \",var_value)).lower().strip()\n",
    "                shell_vars[var] = var_value\n",
    "\n",
    "            if job_var in shell_vars:\n",
    "                job_name = shell_vars[job_var]\n",
    "            else:\n",
    "                job_name = \"\"\n",
    "            if tol_percent_var in shell_vars:\n",
    "                tolerance_percent = shell_vars[tol_percent_var]\n",
    "            else:\n",
    "                tolerance_percent = '0'\n",
    "            \n",
    "            if act_sql_var in shell_vars:\n",
    "                findstring = \"$JOBNAME\"\n",
    "                replacestring = job_name\n",
    "                pattern = re.compile(re.escape(findstring), re.IGNORECASE)\n",
    "                shell_vars[act_sql_var] = pattern.sub(replacestring, shell_vars[act_sql_var])\n",
    "                for findstring, replacestring in pre_process_find_replace_list:                        \n",
    "                    pattern = re.compile(re.escape(findstring), re.IGNORECASE)\n",
    "                    shell_vars[act_sql_var] = pattern.sub(replacestring, shell_vars[act_sql_var])\n",
    "            \n",
    "            if exp_sql_var in shell_vars:\n",
    "                findstring = \"$JOBNAME\"\n",
    "                replacestring = job_name\n",
    "                pattern = re.compile(re.escape(findstring), re.IGNORECASE)\n",
    "                shell_vars[exp_sql_var] = pattern.sub(replacestring, shell_vars[exp_sql_var])\n",
    "                for findstring, replacestring in pre_process_find_replace_list:                        \n",
    "                    pattern = re.compile(re.escape(findstring), re.IGNORECASE)\n",
    "                    shell_vars[exp_sql_var] = pattern.sub(replacestring, shell_vars[exp_sql_var])\n",
    "\n",
    "            # Identify Process Type and capture SQLs\n",
    "            source_sql = \"\"\n",
    "            target_sql = \"\"\n",
    "            \n",
    "            if exp_sql_var in shell_vars and act_sql_var in shell_vars:\n",
    "                source_sql = re.sub(\" +\", \" \",re.sub(\"[\\r]|[\\n]|[\\t]\",\" \",shell_vars[exp_sql_var])).lower().strip()\n",
    "                target_sql = re.sub(\" +\", \" \",re.sub(\"[\\r]|[\\n]|[\\t]\",\" \",shell_vars[act_sql_var])).lower().strip()\n",
    "                \n",
    "                # if any(substring.lower()+\".\" in shell_vars[act_sql_var].lower() for substring in core_views_list):\n",
    "                if regexp_core.search(target_sql):\n",
    "                    process = \"Integration\"\n",
    "                else:\n",
    "                    process = \"Source Table Ingestion\"\n",
    "            else:\n",
    "                process = \"Source File Ingestion\"                \n",
    "                if act_sql_var in shell_vars:\n",
    "                    target_sql = re.sub(\" +\", \" \",re.sub(\"[\\r]|[\\n]|[\\t]\",\" \",shell_vars[act_sql_var])).lower().strip()\n",
    "                \n",
    "            # Create SQL Files with Teradata SQL Script for conversions\n",
    "            # source_select = \"\"\n",
    "            # target_select = \"\"\n",
    "            staging_table = \"\"\n",
    "            if regexp_staging.search(source_sql) and regexp_edw.search(target_sql):\n",
    "                status = \"Create Template\"\n",
    "                # source_select = regexp_select.search(source_sql).group()\n",
    "                # target_select = regexp_select.search(target_sql).group()\n",
    "                conversion_source_exp_file_path = f\"{conversion_source_exp_path_folder}\\{filename}{output_file_extension}\"\n",
    "                write_file_local(conversion_source_exp_file_path, shell_vars[exp_sql_var])\n",
    "                regexp_subquery = re.compile(r\"(?:\"+'|'.join(staging_dataset_list)+\")\\.([a-z|0-9|_]+)\\s*\",re.IGNORECASE | re.DOTALL)\n",
    "                staging_table_find = regexp_subquery.search(shell_vars[exp_sql_var])\n",
    "                if staging_table_find is None:\n",
    "                    staging_table = \"\"\n",
    "                else:\n",
    "                    staging_table = staging_table_find.groups()[0].lower()\n",
    "                \n",
    "                conversion_source_act_file_path = f\"{conversion_source_act_path_folder}\\{filename}{output_file_extension}\"\n",
    "                write_file_local(conversion_source_act_file_path, shell_vars[act_sql_var])\n",
    "            elif regexp_edw.search(target_sql,re.IGNORECASE):\n",
    "                status = \"Convert Target\"\n",
    "                # target_select = regexp_select.search(target_sql).group()\n",
    "                conversion_source_act_file_path = f\"{conversion_source_act_path_folder}\\{filename}{output_file_extension}\"\n",
    "                write_file_local(conversion_source_act_file_path, shell_vars[act_sql_var])\n",
    "            else:\n",
    "                status = \"Ignore All\"\n",
    "\n",
    "            # control_rec_list.append(f\"{job_name}^{filename}^{tolerance_percent}^{process}^{status}\")\n",
    "            control_rec = {'job_name':job_name,'filename':filename,'tolerance_percent':tolerance_percent,'process':process,'status':status,'staging_table':staging_table,'col_count':'0'}\n",
    "            control_rec_list.append(control_rec)\n",
    "            shell_vars.clear()\n",
    "\n",
    "        print(\"Created Expected and Actual Files\") \n",
    "\n",
    "        return control_rec_list\n",
    "\n",
    "    except Exception as e1:\n",
    "        print(e1)\n",
    "        print(\"File Name : {}\".format(filename))\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scan_file_for_shellvariables(file_path, variable_set):\n",
    "    pattern = r'\\$[a-zA-Z_][a-zA-Z0-9_]*'\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                 if not line.strip().startswith('--'):\n",
    "                    variables = re.findall(pattern, line)\n",
    "                    variable_set.update(variables)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file '{file_path}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_source_string(formatted_text):\n",
    "\n",
    "    regexp_source_string = re.compile(r\"SELECT (.*) AS SOURCE_STRING\",re.IGNORECASE)\n",
    "    regexp_cast_varchar = re.compile(r\"(.*)CAST[ ]*\\([ ]*(.*)[ ]+AS VARCHAR[ ]*\\([0-9]*\\)[ ]*\\)(.*)\",re.IGNORECASE)\n",
    "    # regexp_trim = re.compile(r\"(.*)TRIM\\([ ]*(.*)[ ]*\\)(.*)\",re.IGNORECASE)\n",
    "\n",
    "    formatted_text = re.sub(r\"[ ]+\", \" \",formatted_text)\n",
    "\n",
    "    source_string_cols = re.search(regexp_source_string, formatted_text)\n",
    "\n",
    "    col_count = 0\n",
    "\n",
    "    if source_string_cols is not None:\n",
    "        \n",
    "        column_expression = ''.join(source_string_cols.groups())\n",
    "        column_expression_split = column_expression.split(\"||\")\n",
    "        column_expression_split[:] = (value for value in column_expression_split if value.strip() != \"','\")\n",
    "        column_expression_split.pop(0) # Remove First Element which is the job\n",
    "\n",
    "        # column_expression_split[:] = (value.strip() for value in column_expression_split)\n",
    "\n",
    "        # source_string_parsed = \"SELECT CONCAT(\" + ', '.join(column_expression_split) + \") AS SOURCE_STRING\"\n",
    "\n",
    "        column_expr_list = []\n",
    "\n",
    "        col_count = len(column_expression_split)\n",
    "\n",
    "        for sql_expr in column_expression_split:\n",
    "            sqlexp_cast_varchar_match = regexp_cast_varchar.match(sql_expr)\n",
    "            if sqlexp_cast_varchar_match is not None:\n",
    "                sqlexp_novarchar_string = ''.join(sqlexp_cast_varchar_match.groups())\n",
    "                # sqlexp_trim_match = regexp_trim.match(sqlexp_novarchar_string)\n",
    "                # if sqlexp_trim_match is not None:\n",
    "                #     sqlexp_notrim_string = ''.join(sqlexp_trim_match.groups())\n",
    "                #     column_expr_list.append(sqlexp_notrim_string)\n",
    "                # else:\n",
    "                column_expr_list.append(sqlexp_novarchar_string)\n",
    "            else:\n",
    "                # sqlexp_trim_match = regexp_trim.match(sql_expr)\n",
    "                # if sqlexp_trim_match is not None:\n",
    "                #     sqlexp_notrim_string = ''.join(sqlexp_trim_match.groups())\n",
    "                #     column_expr_list.append(sqlexp_notrim_string)\n",
    "                # else:\n",
    "                column_expr_list.append(sql_expr)\n",
    "\n",
    "        column_expr_list[:] = (value.strip() for value in column_expr_list)\n",
    "\n",
    "        if template_version == \"validation_script_template_v2.j2\":\n",
    "            source_string_parsed = \"SELECT CONCAT(\" + ', '.join(column_expr_list) + \") AS SOURCE_STRING\"\n",
    "        elif template_version == \"validation_script_template_v1.j2\":\n",
    "            source_string_parsed = \"SELECT \" + ', '.join(column_expr_list)\n",
    "\n",
    "        formatted_text_min = formatted_text.replace(source_string_cols.group().strip(), source_string_parsed)\n",
    "        \n",
    "    else:\n",
    "        formatted_text_min = formatted_text\n",
    "    \n",
    "    return formatted_text_min, col_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sql_file(source_file_path, copy_file_path, pre_process_find_replace_list):\n",
    "        \n",
    "        # preferred_encodings = [\"cp1252\", \"utf-8\", \"iso-8859-1\"]\n",
    "        preferred_encodings = [\"cp1252\"]\n",
    "        for encoding in preferred_encodings:\n",
    "            try:\n",
    "                lines = read_contents_file(source_file_path, encoding, True)\n",
    "            except UnicodeDecodeError as e:\n",
    "                print(f\"UnicodeDecodeError with {encoding}: {e}\")\n",
    "                continue\n",
    "                 \n",
    "        sqltext = ''\n",
    "        regexp_cast_varchar = re.compile(r\"(.*)CAST[ ]*\\([ ]*(.*)[ ]+AS VARCHAR[ ]*\\([0-9]*\\)[ ]*\\)(.*)\",re.IGNORECASE)\n",
    "        \n",
    "        for line in lines:\n",
    "            for findstring, replacestring in pre_process_find_replace_list:                        \n",
    "                pattern = re.compile(re.escape(findstring), re.IGNORECASE)\n",
    "                line = pattern.sub(replacestring, line)\n",
    "                    \n",
    "            sqlexp_cast_varchar_match = regexp_cast_varchar.match(line)\n",
    "\n",
    "            if sqlexp_cast_varchar_match is not None:\n",
    "                line = ''.join(sqlexp_cast_varchar_match.groups())\n",
    "            \n",
    "            if \"EOF\"  in line \\\n",
    "                or line.strip().startswith('#') \\\n",
    "                    or line.lower().strip().startswith('locking') :\n",
    "                    line = f\"--{line}\"\n",
    "                \n",
    "            sqltext += line\n",
    "                \n",
    "        formattedtext = sqlparse.format(sqltext, reindent=True, keyword_case='upper', strip_comments=False)\n",
    "\n",
    "        formattedtext, col_count = preprocess_source_string(formattedtext)\n",
    "\n",
    "        write_file_local(copy_file_path,formattedtext)\n",
    "        # print(\"Removed shell variables, Shell Comments, Collect Stats, EOF & Renamed  {}\".format(os.path.basename(copy_file_path)))\n",
    "\n",
    "        return col_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sqls(conversion_source_path_folder, preprocessed_tdsqlfiles_path, sub_folder_list, control_rec_list):\n",
    "\n",
    "    print(\"Pre Processing has begun\") \n",
    "    \n",
    "    for folder in sub_folder_list:\n",
    "        sub_folder = f\"{preprocessed_tdsqlfiles_path}\\{folder}\"\n",
    "        if os.path.exists(sub_folder):\n",
    "            shutil.rmtree(sub_folder, ignore_errors=True)\n",
    "        os.makedirs(sub_folder)\n",
    "        print(\"Created folder : {}\".format(sub_folder))\n",
    "    \n",
    "    for zip_file in metadatazip_file_list:\n",
    "        metadatazip_file = os.path.join(metadata_folder, zip_file)\n",
    "\n",
    "        if not os.path.isfile(metadatazip_file):\n",
    "            raise FileNotFoundError(f\"Metadata Zip file '{metadatazip_file}' not found.\")\n",
    "            print(\"Metadata Zip file '{metadatazip_file}' not found.\")\n",
    "        else:\n",
    "            shutil.copy2(metadatazip_file, preprocessed_tdsqlfiles_path)\n",
    "            print(\"Copied metadatazip_file {}\".format(metadatazip_file))\n",
    "        \n",
    "    yaml_file_list = glob.glob(os.path.join(metadata_folder  , '*.yaml' ))\n",
    "\n",
    "    for yaml_file_path in yaml_file_list:\n",
    "        filename = os.path.basename(yaml_file_path)\n",
    "        destination_path = os.path.join(preprocessed_tdsqlfiles_path, filename)\n",
    "        shutil.copy2(yaml_file_path, destination_path)\n",
    "        print(\"Copied yaml file {}\".format(filename))\n",
    "\n",
    "    print(\"Pre Processing is completed\") \n",
    "\n",
    "    shell_variables = set()\n",
    "    #create empty set for holding shell variables\n",
    "\n",
    "    for folder in sub_folder_list:\n",
    "        conversion_source_sub_path_folder = f\"{conversion_source_path_folder}\\{folder}\"\n",
    "        for file in os.listdir(conversion_source_sub_path_folder):\n",
    "            if file.endswith('.sql'):\n",
    "                conversion_source_file_path = os.path.join(conversion_source_sub_path_folder, file)\n",
    "                scan_file_for_shellvariables(conversion_source_file_path, shell_variables)\n",
    "                preprocess_file_path = os.path.join(preprocessed_tdsqlfiles_path, folder, file).lower()\n",
    "                col_count = preprocess_sql_file(conversion_source_file_path, preprocess_file_path, pre_process_find_replace_list)\n",
    "                idx = next((idx for idx, item in enumerate(control_rec_list) if item[\"filename\"].lower() == file.split(\".\")[0].lower()), None)\n",
    "                control_rec_list[idx]['col_count'] = str(col_count)\n",
    "    \n",
    "    print(\"Printing any shell variables found in sqls\")\n",
    "    for variable in shell_variables:\n",
    "            print(variable)\n",
    "\n",
    "    return control_rec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bqms_translator(bqms_in_folder, convertedsql_path, sub_folder_list, bucket_name, gcsfolder, run_time, v_mode):\n",
    "\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "\n",
    "    gcs_input_path = gcsfolder + '/' + run_time + '/input' \n",
    "    gcs_converted_path = gcsfolder + '/' +   run_time + '/output'\n",
    "    translation_display_name = 'Validation_Conversion_' + run_time\n",
    "\n",
    "    print(\"Running {} BQMS Translation {}\".format(v_mode, translation_display_name))\n",
    "\n",
    "    print(\"Uploading Preprocessed files\")\n",
    "\n",
    "    command = 'gsutil -m cp -r ' + bqms_in_folder + '\\ gs://' + bucket_name  + '/' +  gcs_input_path \n",
    "\n",
    "    try:\n",
    "        completed_process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(completed_process.stdout)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(e.output)\n",
    "\n",
    "    print(f\"Begin: All Objects Uploaded, Now Create a Migration Workflow\") \n",
    "\n",
    "    parent = f\"projects/{translation_project_id}/locations/us\"\n",
    "\n",
    "    client = bigquery_migration_v2.MigrationServiceClient()\n",
    "    source_dialect = bigquery_migration_v2.Dialect()\n",
    "\n",
    "    if v_mode == 'BTEQ':\n",
    "        source_dialect.teradata_dialect = bigquery_migration_v2.TeradataDialect(\n",
    "            mode=bigquery_migration_v2.TeradataDialect.Mode.BTEQ\n",
    "        )\n",
    "    elif v_mode == 'SQL':\n",
    "        source_dialect.teradata_dialect = bigquery_migration_v2.TeradataDialect(\n",
    "            mode=bigquery_migration_v2.TeradataDialect.Mode.SQL\n",
    "        )\n",
    "    else:\n",
    "        raise Exception(f\"Unknown Mode {v_mode}.\")\n",
    "    target_dialect = bigquery_migration_v2.Dialect()\n",
    "    target_dialect.bigquery_dialect = bigquery_migration_v2.BigQueryDialect()\n",
    "    \n",
    "    translation_config = bigquery_migration_v2.TranslationConfigDetails(\n",
    "        gcs_source_path= \"gs://\" + bucket_name + '/' + gcs_input_path,\n",
    "        gcs_target_path=\"gs://\" + bucket_name + '/' + gcs_converted_path,\n",
    "        source_dialect=source_dialect,\n",
    "        target_dialect=target_dialect,\n",
    "        source_env = bigquery_migration_v2.SourceEnv(default_database=project_id, schema_search_path=lob_datasets)\n",
    "        \n",
    "    )\n",
    "\n",
    "    migration_task = bigquery_migration_v2.MigrationTask(\n",
    "        type_=\"Translation_Teradata2BQ\", translation_config_details=translation_config\n",
    "    )\n",
    "\n",
    "    workflow = bigquery_migration_v2.MigrationWorkflow(\n",
    "        display_name=translation_display_name\n",
    "    )\n",
    "\n",
    "    workflow.tasks[\"translation-task\"] = migration_task  \n",
    "\n",
    "    request = bigquery_migration_v2.CreateMigrationWorkflowRequest(\n",
    "        parent=parent,\n",
    "        migration_workflow=workflow,\n",
    "    )\n",
    "    response = client.create_migration_workflow(request=request)\n",
    "    print(\"State : \" + str(response.state))\n",
    "\n",
    "    # while str(response.state) != \"State.COMPLETED\":\n",
    "    while str(response.state) != \"4\":\n",
    "        time.sleep(5)\n",
    "        response = client.get_migration_workflow(name=response.name)  \n",
    "        # print(str(response.state))\n",
    "    \n",
    "    print(\"Migration Workflow is completed\")\n",
    "\n",
    "    for folder in sub_folder_list:\n",
    "        sub_folder = f\"{convertedsql_path}\\{folder}\"\n",
    "        if os.path.exists(sub_folder):\n",
    "            shutil.rmtree(sub_folder, ignore_errors=True)\n",
    "        os.makedirs(sub_folder)\n",
    "        print(\"Created folder : {}\".format(sub_folder))\n",
    "\n",
    "        command = f'gsutil -m cp gs://{bucket_name}/{gcs_converted_path}/{folder}/*.sql {convertedsql_path}\\{folder}'\n",
    "\n",
    "        print(\"Downloaded Translated files from {}\".format(f\"{bucket_name}/{gcs_converted_path}/{folder}\"))\n",
    "\n",
    "        try:\n",
    "            completed_process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            print(completed_process.stdout)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(\"Error:\", e)\n",
    "            print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_bqsqls(convertedsql_path, postprocess_sql_path, sub_folder_list):\n",
    "\n",
    "    print(\"Post Processing has begun\") \n",
    "    \n",
    "    for folder in sub_folder_list:\n",
    "        sub_folder = f\"{postprocess_sql_path}\\{folder}\"\n",
    "        if os.path.exists(sub_folder):\n",
    "            shutil.rmtree(sub_folder, ignore_errors=True)\n",
    "        os.makedirs(sub_folder)\n",
    "        print(\"Created folder : {}\".format(sub_folder))\n",
    "\n",
    "        file_list = [filename for filename in os.listdir(f\"{convertedsql_path}\\{folder}\") if filename.endswith(\".sql\")]\n",
    "        for filename in file_list:\n",
    "            source_file_path = os.path.join(convertedsql_path, folder, filename).lower()\n",
    "            converted_sql = read_contents_file(source_file_path)\n",
    "\n",
    "            for i in range(len(post_process_find_replace_list)):\n",
    "                converted_sql = converted_sql.replace(post_process_find_replace_list[i][\"search\"], post_process_find_replace_list[i]['replace'])\n",
    "\n",
    "            for i in range(len(post_process_regex_find_replace_list)):\n",
    "                regexp_pattern = re.compile(post_process_regex_find_replace_list[i][\"search\"], re.IGNORECASE)\n",
    "                regexp_repl_pattern = post_process_regex_find_replace_list[i][\"replace\"]\n",
    "                converted_sql = re.sub(regexp_pattern, regexp_repl_pattern, converted_sql)\n",
    "\n",
    "            formattedsql = converted_sql.strip()\n",
    "            formattedsql = sqlparse.format(formattedsql, reindent=True, keyword_case='upper')\n",
    "\n",
    "            for i in range(len(post_process_find_replace_list)):\n",
    "                formattedsql = formattedsql.replace(post_process_find_replace_list[i][\"search\"], post_process_find_replace_list[i]['replace'])  \n",
    "\n",
    "            for i in range(len(post_process_regex_find_replace_list)):\n",
    "                regexp_pattern = re.compile(post_process_regex_find_replace_list[i][\"search\"], re.IGNORECASE)\n",
    "                regexp_repl_pattern = post_process_regex_find_replace_list[i][\"replace\"]\n",
    "                formattedsql = re.sub(regexp_pattern, regexp_repl_pattern, formattedsql)\n",
    "\n",
    "            formattedsql = formattedsql.strip(' ;')\n",
    "            copy_file_path = os.path.join(postprocess_sql_path, folder, filename).lower()\n",
    "            # print(copy_file_path)\n",
    "            write_file_local(copy_file_path,formattedsql)\n",
    "\n",
    "    print(\"Post Processing is completed\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_sql(postprocess_sql_path, sub_folder_list):\n",
    "    print(f\"executing SQL files in  {postprocess_sql_path}\" )\n",
    "    client = bigquery.Client(project=project_id)\n",
    "\n",
    "    for folder in sub_folder_list:\n",
    "        sub_folder = f\"{postprocess_sql_path}\\{folder}\"\n",
    "        for file_name in os.listdir(sub_folder):\n",
    "            if file_name.endswith('.sql'):\n",
    "                sql_file_path = os.path.join(sub_folder, file_name)\n",
    "                sql_query = read_contents_file(sql_file_path)\n",
    "                for i in range(len(execution_time_find_replace_list)):\n",
    "                    sql_query = sql_query.replace(execution_time_find_replace_list[i][\"search\"], execution_time_find_replace_list[i]['replace'])\n",
    "                try:         \n",
    "                    client.query(sql_query, project=project_id, location='US').result()\n",
    "                    logging.info(f\"SQL file {file_name} executed successfully.\")\n",
    "                    print(f\"SQL file {file_name} executed successfully.\")\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error executing SQL file {file_name}: {e}\")\n",
    "                    print(f\"Error executing SQL file {file_name}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_execution_results(logfilename):\n",
    "    total_lines = 0\n",
    "    success_lines = 0\n",
    "    failure_lines = 0\n",
    "    missing_dbobjects = 0 \n",
    "    dup_data_issues = 0\n",
    "\n",
    "    try:\n",
    "        with open(logfilename, \"r\") as file:\n",
    "            print(\"=========Errors================\")\n",
    "            for line in file:\n",
    "                if \"SQL file\" in line:\n",
    "                    total_lines += 1\n",
    "                    if \"executed successfully\" in line:\n",
    "                        success_lines += 1\n",
    "                    elif \"Not found\" in line:\n",
    "                        missing_dbobjects  += 1    \n",
    "                        print(line.strip())                    \n",
    "                        failure_lines += 1\n",
    "                    elif \"Duplicates\" in line or \"at most one source row\" in line :\n",
    "                        dup_data_issues  += 1    \n",
    "                        print(line.strip())                    \n",
    "                        failure_lines += 1\n",
    "                    else:\n",
    "                        failure_lines += 1\n",
    "                        print(line.strip())\n",
    "\n",
    "        print(\"=========execution_results================\")\n",
    "        print(\"Total SQL files executed:\", total_lines)\n",
    "        print(\"SQL files executed successfully:\", success_lines)\n",
    "        print(\"SQL files failed with errors:\", failure_lines)\n",
    "        print(\" --SQL files with Missing db objects:\", missing_dbobjects)\n",
    "        print(\" --SQL files with Dup Data Issues:\", dup_data_issues)\n",
    "\n",
    "\n",
    "        logging.info(\"=========execution_results================\")\n",
    "        logging.info(f\"Total SQL files executed: {total_lines}\")\n",
    "        logging.info(f\"SQL files executed successfully: {success_lines}\")\n",
    "        logging.info(f\"SQL files failed with errors: {failure_lines}\")\n",
    "        logging.info(f\" --SQL files with Missing db objects: {missing_dbobjects}\")\n",
    "        logging.info(f\" --SQL files with Dup Data Issues: {dup_data_issues}\")\n",
    "                     \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{logfilename}' not found.\")\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def productionize_sqls(postprocessqls, prodready_bqsqls):\n",
    "\n",
    "    for folder in sub_folder_list:\n",
    "        sub_folder = f\"{prodready_bqsqls}\\{folder}\"\n",
    "        if os.path.exists(sub_folder):\n",
    "            shutil.rmtree(sub_folder, ignore_errors=True)\n",
    "        os.makedirs(sub_folder)\n",
    "        print(\"Created folder : {}\".format(sub_folder))\n",
    "    \n",
    "        for file in os.listdir(f\"{postprocessqls}\\{folder}\"):\n",
    "            if file.endswith('.sql'):\n",
    "                source_file_path = os.path.join(postprocessqls, folder, file)\n",
    "                post_process_sql = read_contents_file(source_file_path)\n",
    "\n",
    "                for i in range(len(production_regex_find_replace_list)):\n",
    "                    if production_regex_find_replace_list[i][\"query_type\"].lower() == folder.lower():\n",
    "                        regexp_pattern = re.compile(production_regex_find_replace_list[i][\"search\"], re.IGNORECASE | re.DOTALL)\n",
    "                        regexp_repl_pattern = production_regex_find_replace_list[i][\"replace\"]\n",
    "                        post_process_sql = re.sub(regexp_pattern, regexp_repl_pattern, post_process_sql)\n",
    "\n",
    "                for i in range(len(production_parms_find_replace_list)):\n",
    "                    post_process_sql = post_process_sql.replace(production_parms_find_replace_list[i][\"search\"], production_parms_find_replace_list[i][\"replace\"])  \n",
    "\n",
    "                formattedsql = post_process_sql.strip()\n",
    "                formattedsql = sqlparse.format(formattedsql, reindent=True, keyword_case='upper')\n",
    "                \n",
    "                formattedtext = sqlparse.format(formattedsql, reindent=True, keyword_case='upper', strip_comments=False)\n",
    "                copy_file_path = os.path.join(prodready_bqsqls, folder, file).lower()\n",
    "                write_file_local(copy_file_path, formattedtext)\n",
    "                # print(\"Replaced hardcoded schema names with Dag Parms  {}\".format(os.path.basename(copy_file_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_dags(output_path_folder):\n",
    "    # print(prodready_bqsqls)\n",
    "    command = 'gsutil -m cp -r ' + output_path_folder + '\\*\\ gs://' + dag_bucket_path\n",
    "\n",
    "    try:\n",
    "        completed_process = subprocess.run(command, shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        print(completed_process.stdout)\n",
    "        print(\"Uploaded Dags to {}\".format(dag_bucket_path))\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(\"Error:\", e)\n",
    "        print(e.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_validation_sqls_from_template(control_rec_list):\n",
    "    ############################ POST CONVERSION + TEMPLATE CREATIONS ############################\n",
    "\n",
    "    if os.path.exists(output_path_folder):\n",
    "        shutil.rmtree(output_path_folder, ignore_errors=True)\n",
    "    os.makedirs(output_path_folder)\n",
    "    print(\"Created folder : {}\".format(output_path_folder))\n",
    "    \n",
    "    # Jinja2 Environment\n",
    "    env = Environment(loader = FileSystemLoader(jinja_template_path), trim_blocks=True, lstrip_blocks=True)\n",
    "    template_filename = template_version\n",
    "    template = env.get_template(template_filename)\n",
    "\n",
    "    for idx, rec in enumerate(control_rec_list):\n",
    "        job_name = rec['job_name']\n",
    "        filename = rec['filename']\n",
    "        tolerance_percent = rec['tolerance_percent']\n",
    "        process = rec['process']\n",
    "        status = rec['status']\n",
    "        staging_table = rec['staging_table']\n",
    "        col_count = rec['col_count']\n",
    "\n",
    "        df_job_source = pd.read_csv(\"config\\job_source_map.csv\", index_col=None)\n",
    "        df_job_source_match = df_job_source[df_job_source[\"job_name\"]==filename]\n",
    "        if df_job_source_match.empty:\n",
    "            source_per_job_name = \"\"\n",
    "        else:\n",
    "            source_per_job_name = df_job_source_match[\"source\"].iloc[0]\n",
    "\n",
    "        if status == \"Create Template\":\n",
    "            template_params = dict()\n",
    "            template_params.update({\"stage_dataset\": '{{ params.param_' + lob_abbr_lower + '_stage_dataset_name }}'})\n",
    "            template_params.update({\"target_dataset\": '{{ params.param_' + lob_abbr_lower + '_core_dataset_name }}'})\n",
    "            template_params.update({\"audit_dataset\": '{{ params.param_' + lob_abbr_lower + '_audit_dataset_name }}'})\n",
    "            template_params.update({\"tol_percent\": tolerance_percent})\n",
    "            template_params.update({\"source_per_job_name\": source_per_job_name})\n",
    "            template_params.update({\"staging_table_name\": staging_table})\n",
    "            \n",
    "            conversion_target_exp_file_path = f\"{prodready_exp_path_folder}\\{filename}{output_file_extension}\"\n",
    "            conv_source_sql = read_contents_file(conversion_target_exp_file_path)\n",
    "            formattedtext = sqlparse.format(conv_source_sql, reindent=True, keyword_case='upper', strip_comments=True)\n",
    "            template_params.update({\"src_query\": formattedtext})\n",
    "\n",
    "            conversion_target_act_file_path = f\"{prodready_act_path_folder}\\{filename}{output_file_extension}\"\n",
    "            conv_target_sql = read_contents_file(conversion_target_act_file_path)\n",
    "            formattedtext = sqlparse.format(conv_target_sql, reindent=True, keyword_case='upper', strip_comments=True)\n",
    "            template_params.update({\"tgt_query\": formattedtext})\n",
    "\n",
    "            if template_version == \"validation_script_template_v2.j2\":\n",
    "                match process:\n",
    "                    case 'Integration':\n",
    "                        template_params.update({\"variable_prefix\": \"VALIDATION_CNTRLID_\"})\n",
    "                    case 'Source Table Ingestion':\n",
    "                        template_params.update({\"variable_prefix\": \"INGEST_CNTRLID_\"})\n",
    "            elif template_version == \"validation_script_template_v1.j2\":\n",
    "                control_count = int(col_count)\n",
    "                control_ids = []\n",
    "                for count_id in range(1,control_count+1):\n",
    "                    control_rec = {\"id\":count_id}\n",
    "                    control_ids.append(control_rec)\n",
    "                template_params.update({\"control_ids\": control_ids})\n",
    "            \n",
    "            os.makedirs(f\"{output_path_folder}\\{source_per_job_name}\", exist_ok=True)\n",
    "            output_filename_path = f\"{output_path_folder}\\{source_per_job_name}\\{filename}{output_file_extension}\"\n",
    "            write_file_local(output_filename_path, template.render(template_params) + \"\\n\")\n",
    "            template_params.clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_control_file(control_rec_list):\n",
    "    if not os.path.exists(output_controlpath_folder):\n",
    "        os.makedirs(output_controlpath_folder)\n",
    "        print(\"Created folder {}\".format(output_controlpath_folder))\n",
    "\n",
    "\n",
    "    control_rec_list_flatten = ['^'.join(rec.values()) for rec in control_rec_list]\n",
    "\n",
    "    # Capture Summary of the Audit Control\n",
    "    write_file_local(output_control_file_path, control_rec_list_flatten, True)\n",
    "    print(\"Wrote Control File {}\".format(output_control_file_path)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt1 = datetime.now()\n",
    "# run_time = (dt1).strftime('%Y%m%d_%H%M')\n",
    "run_time = \"20240906_0352\"\n",
    "\n",
    "with open('config/lob_config.json') as json_lob_config:\n",
    "    config = json.load(json_lob_config)\n",
    "\n",
    "lob = config['lob']\n",
    "lob_abbr = config['lob_abbr'] # lobname in BQMS Script\n",
    "\n",
    "lob_lower = lob.strip().lower()\n",
    "lob_upper = lob.strip().upper()\n",
    "lob_abbr_lower = lob_abbr.strip().lower()\n",
    "lob_abbr_upper = lob_abbr.strip().upper()\n",
    "\n",
    "parmfiles_folder = config['parmfiles_path']\n",
    "parmfiles_path_folder = f\"{parmfiles_folder}\\{lob_abbr_lower}\"\n",
    "\n",
    "output_folder = config['output_folder']\n",
    "output_path_folder = f\"{output_folder}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "output_control_folder = config['output_control_folder']\n",
    "output_controlpath_folder = f\"{output_control_folder}\\{lob_abbr_lower}\"\n",
    "output_control_file_path = f\"{output_controlpath_folder}\\{lob_lower}_{run_time}.csv\"\n",
    "\n",
    "log_folder = config['log_folder']\n",
    "log_path_folder = f\"{log_folder}\\{lob_abbr_lower}\"\n",
    "\n",
    "if not os.path.exists(log_path_folder):\n",
    "    os.makedirs(log_path_folder)\n",
    "\n",
    "logfilename = lob_abbr_lower + '_bqsqlexecution_' + run_time + '.log'\n",
    "logfilenamepath = os.path.join(log_path_folder, logfilename)\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "logging.basicConfig(filename=logfilenamepath, level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "expected_folder = config['expected_folder']\n",
    "actual_folder = config['actual_folder']\n",
    "sub_folder_list = config['sub_folder_list']\n",
    "\n",
    "conversion_source_folder = config['conversion_source_folder']\n",
    "conversion_source_path_folder = f\"{conversion_source_folder}\\{lob_abbr_lower}\\{run_time}\"\n",
    "conversion_source_exp_path_folder = f\"{conversion_source_path_folder}\\{expected_folder}\"\n",
    "conversion_source_act_path_folder = f\"{conversion_source_path_folder}\\{actual_folder}\"\n",
    "\n",
    "conversion_target_folder = config['conversion_target_folder']\n",
    "conversion_target_path_folder = f\"{conversion_target_folder}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "preprocessed_tdsqlfiles = config['preprocessed_tdsqlfiles']\n",
    "preprocessed_tdsqlfiles_path = f\"{preprocessed_tdsqlfiles}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "postprocessed_bqsqlsfiles = config['postprocessqls']\n",
    "postprocessed_bqsqlsfiles_path = f\"{postprocessed_bqsqlsfiles}\\{lob_abbr_lower}\\{run_time}\"\n",
    "\n",
    "prodready_bqsqls = config['prodready_bqsqls']\n",
    "prodready_bqsqls_path = f\"{prodready_bqsqls}\\{lob_abbr_lower}\\{run_time}\"\n",
    "prodready_exp_path_folder = f\"{prodready_bqsqls_path}\\{expected_folder}\"\n",
    "prodready_act_path_folder = f\"{prodready_bqsqls_path}\\{actual_folder}\"\n",
    "\n",
    "output_file_extension = config['output_file_extension']\n",
    "\n",
    "jinja_template_path = config['jinja_template_path']\n",
    "template_version = config['template_version']\n",
    "\n",
    "exp_sql_var = config['exp_sql_var']\n",
    "act_sql_var = config['act_sql_var']\n",
    "job_var = config['job_var']\n",
    "tol_percent_var = config['tol_percent_var']\n",
    "tol_amount_var = config['tol_amount_var']\n",
    "\n",
    "project_id = config['project_id']\n",
    "translation_project_id = config['translation_project_id']\n",
    "fns_project_id = project_id\n",
    "fns_dataset = config['fns_dataset']\n",
    "bucket_name = config['bucket_name']\n",
    "gcsfolder = config['gcsfolder']\n",
    "dag_bucket_path = config['dag_bucket_path']\n",
    "\n",
    "source_dataset_list = config['source_dataset_list']\n",
    "staging_dataset_list = config['staging_dataset_list']\n",
    "core_dataset_list = config['core_dataset_list']\n",
    "core_views_list = config['core_views_list']\n",
    "\n",
    "lob_datasets = config['lob_datasets']\n",
    "\n",
    "unix_server = config['unix_server']\n",
    "username = config['username']\n",
    "password = config['password']\n",
    "\n",
    "unix_etl_folder = config['unix_etl_folder']\n",
    "unix_lob = config['unix_lob']\n",
    "lob_sub_folders = config['lob_sub_folders']\n",
    "parmfolder_before_lob_subfolder = config['parmfolder_before_lob_subfolder']\n",
    "file_prefix = config['file_prefix']\n",
    "add_source_dir_prefix = config['add_source_dir_prefix']\n",
    "\n",
    "metadata_folder = config['metadata_folder']\n",
    "metadatazip_file_list = config['metadatazip_file_list']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # PBS Replacements\n",
    "\n",
    "    # pre_process_find_replace_list = [\n",
    "    #     ('$NCR_TGT_SCHEMA','edwpbs'),\n",
    "    #     ('$NCR_STG_SCHEMA','edwpbs_staging'),\n",
    "    #     ('$NCR_STG_TGT','edwpbs_staging')  ,\n",
    "    #     ('$NCR_BASE_VIEWS','edwpbs_base_views')  ,\n",
    "    #     ('$NCR_TGT','edwpbs') , \n",
    "    #     ('!=','<>') , \n",
    "    #     (\"','AS\", \"',' AS\"),\n",
    "    # ]\n",
    "\n",
    "    # post_process_find_replace_list = [\n",
    "    #     {'search':'syslib.length','replace':'length'},\n",
    "    #     {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-parallon`.bqutil_fns.isnumeric'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwfs_base_views.','replace':'`hca-hin-dev-cur-parallon`.auth_base_views.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwfs.','replace':'`hca-hin-dev-cur-parallon`.auth_base_views.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_base_views.fact_rcom_pars_discrepancy','replace':'`hca-hin-dev-cur-parallon`.edwpbs_base_views.fact_rcom_pars_discrepancy'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_base_views.','replace':'`hca-hin-dev-cur-parallon`.auth_base_views.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_views.','replace':'`hca-hin-dev-cur-parallon`.auth_base_views.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf.','replace':'`hca-hin-dev-cur-parallon`.auth_base_views.'},\n",
    "    # ]\n",
    "\n",
    "    # post_process_regex_find_replace_list = [\n",
    "    #     {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    #     'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    #     {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-parallon`.bqutil_fns.\"}\n",
    "    # ]\n",
    "\n",
    "    # execution_time_find_replace_list = [\n",
    "    # #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    # ]\n",
    "\n",
    "    # production_parms_find_replace_list = [\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpbs_staging.','replace':'{{ params.param_pbs_stage_dataset_name }}.'},        \n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpbs.','replace':'{{ params.param_pbs_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpbs_base_views.','replace':'{{ params.param_pbs_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpbs_views.','replace':'{{ params.param_pbs_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.bqutil_fns.','replace':'{{ params.param_pbs_bqutil_fns_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_staging.','replace':'{{ params.param_pf_stage_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.auth_base_views.','replace':'{{ params.param_auth_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf.','replace':'{{ params.param_pf_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_base_views.','replace':'{{ params.param_pf_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwpf_views.','replace':'{{ params.param_pf_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwra_staging.','replace':'{{ params.param_ra_stage_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwra_base_views.','replace':'{{ params.param_ra_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwra_views.','replace':'{{ params.param_ra_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edw_dim_base_views.','replace':'{{ params.param_dim_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'{{ params.param_pub_views_dataset_name }}.'},      \n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwfs.','replace':'{{ params.param_fs_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwfs_base_views.','replace':'{{ params.param_fs_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-parallon`.edwcm_views.','replace':'{{ params.param_cm_views_dataset_name }}.'},\n",
    "    # ]\n",
    "\n",
    "    # production_regex_find_replace_list = [\n",
    "    #     {'search':r\"((?:\"+'|'.join(core_dataset_list)+\")\\.[a-z|0-9|_]+[ ]*[AS]*[ ]*(?!ON)[a-z|0-9|_]*)\",\n",
    "    #     'replace':r\"\\1 FOR SYSTEM_TIME AS OF TIMESTAMP(tableload_start_time,'US/Central')\",\n",
    "    #     'query_type':\"EXP\"},\n",
    "    #     {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)[ ]*=[ ]*current_date\\('US\\/Central'\\)[ ]*\", \n",
    "    #     'replace':r\"\\1 >= tableload_start_time  - INTERVAL 1 MINUTE \",\n",
    "    #     'query_type':\"ACT\"}\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # MHB Replacements\n",
    "\n",
    "    # pre_process_find_replace_list = [\n",
    "    #     ('$NCR_TGT_SCHEMA','edwci'),\n",
    "    #     ('$NCR_STG_SCHEMA','edwci_staging'),\n",
    "    #     ('$NCR_STG_TGT','edwci_staging')  ,\n",
    "    #     ('$NCR_BASE_VIEWS','edwci_base_views')  ,\n",
    "    #     ('$NCR_TGT','edwci') , \n",
    "    #     ('!=','<>') , \n",
    "    #     (\"','AS\", \"',' AS\"),\n",
    "    # ]\n",
    "\n",
    "    # post_process_find_replace_list = [\n",
    "    #     {'search':'syslib.length','replace':'length'},\n",
    "    #     {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-clinical`.bqutil_fns.isnumeric'},\n",
    "    #     # {'search':'`hca-hin-dev-cur-clinical`.edwfs_base_views.','replace':'`hca-hin-dev-cur-clinical`.auth_base_views.'},\n",
    "    # ]\n",
    "\n",
    "    # post_process_regex_find_replace_list = [\n",
    "    #     {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    #     'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    #     {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-clinical`.bqutil_fns.\"}\n",
    "    # ]\n",
    "\n",
    "    # execution_time_find_replace_list = [\n",
    "    # #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    # ]\n",
    "\n",
    "    # production_parms_find_replace_list = [\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwci_staging.','replace':'{{ params.param_clinical_ci_stage_dataset_name }}.'},        \n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwci.','replace':'{{ params.param_clinical_ci_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwci_base_views.','replace':'{{ params.param_clinical_ci_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.bqutil_fns.','replace':'{{ params.param_clinical_bqutil_fns_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.auth_base_views.','replace':'{{ params.param_clinical_cdm_auth_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcl_base_views.','replace':'{{ params.param_clinical_cl_base_views_dataset_name }}.'}\n",
    "    # ]\n",
    "\n",
    "    # production_regex_find_replace_list = [\n",
    "    #     {'search':r\"((?:\"+'|'.join(core_dataset_list)+\")\\.[a-z|0-9|_]+[ ]*[AS]*[ ]*(?!ON)[a-z|0-9|_]*)\",\n",
    "    #     'replace':r\"\\1 FOR SYSTEM_TIME AS OF TIMESTAMP(tableload_start_time,'US/Central')\",\n",
    "    #     'query_type':\"EXP\"},\n",
    "    #     {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)[ ]*=[ ]*current_date\\('US\\/Central'\\)[ ]*\", \n",
    "    #     'replace':r\"\\1 >= tableload_start_time  - INTERVAL 1 MINUTE \",\n",
    "    #     'query_type':\"ACT\"}\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # # CA Replacements\n",
    "\n",
    "    # pre_process_find_replace_list = [\n",
    "    #     ('$NCR_TGT_SCHEMA','edwcdm'),\n",
    "    #     ('$NCR_STG_SCHEMA','edwcdm_staging'),\n",
    "    #     ('$NCR_STG_TGT','edwcdm_staging')  ,\n",
    "    #     ('$NCR_BASE_VIEWS','edwcdm_base_views')  ,\n",
    "    #     ('$NCR_TGT','edwcdm') , \n",
    "    #     ('!=','<>') , \n",
    "    #     (\"','AS\", \"',' AS\"),\n",
    "    # ]\n",
    "\n",
    "    # post_process_find_replace_list = [\n",
    "    #     {'search':'syslib.length','replace':'length'},\n",
    "    #     {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-clinical`.bqutil_fns.isnumeric'},\n",
    "    # ]\n",
    "\n",
    "    # post_process_regex_find_replace_list = [\n",
    "    #     {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    #     'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    #     {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-clinical`.bqutil_fns.\"}\n",
    "    # ]\n",
    "\n",
    "    # execution_time_find_replace_list = [\n",
    "    # #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    # ]\n",
    "\n",
    "    # production_parms_find_replace_list = [\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcdm_staging.','replace':'{{ params.param_clinical_cdm_stage_dataset_name }}.'},        \n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcdm.','replace':'{{ params.param_clinical_cdm_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcdm_base_views.','replace':'{{ params.param_clinical_cdm_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.bqutil_fns.','replace':'{{ params.param_clinical_bqutil_fns_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.auth_base_views.','replace':'{{ params.param_clinical_cdm_auth_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcl_base_views.','replace':'{{ params.param_clinical_cl_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-clinical`.edwcdm_views.','replace':'{{ params.param_clinical_cdm_views_dataset_name }}.'}\n",
    "    # ]\n",
    "\n",
    "    # production_regex_find_replace_list = [\n",
    "    #     {'search':r\"((?:\"+'|'.join(core_dataset_list)+\")\\.[a-z|0-9|_]+[ ]*[AS]*[ ]*(?!ON)[a-z|0-9|_]*)\",\n",
    "    #     'replace':r\"\\1 FOR SYSTEM_TIME AS OF TIMESTAMP(tableload_start_time,'US/Central')\",\n",
    "    #     'query_type':\"EXP\"},\n",
    "    #     {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)[ ]*=[ ]*current_date\\('US\\/Central'\\)[ ]*\", \n",
    "    #     'replace':r\"\\1 >= tableload_start_time  - INTERVAL 1 MINUTE \",\n",
    "    #     'query_type':\"ACT\"}\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # CR Replacements\n",
    "\n",
    "    # pre_process_find_replace_list = [\n",
    "    #     ('$NCR_TGT_SCHEMA','edwcr'),\n",
    "    #     ('${NCR_TGT_SCHEMA}','edwcr') ,\n",
    "    #     ('$NCR_STG_SCHEMA','edwcr_staging'),\n",
    "    #     ('${NCR_STG_SCHEMA}','edwcr_staging'),\n",
    "    #     ('$NCR_STG_TGT','edwcr_staging')  ,\n",
    "    #     ('$NCR_BASE_VIEWS','edwcr_base_views')  ,\n",
    "    #     ('$EDWCR_BASE_VIEWS','edwcr_base_views')  ,\n",
    "    #     ('$NCR_TGT','edwcr') ,\n",
    "    #     ('$NCR_AC_VIEW','edwcr_dmx_ac_base_views') ,\n",
    "    #     ('${NCR_AC_VIEW}', 'edwcr_dmx_ac_base_views') ,\n",
    "    #     ('$NCR_AC_SCHEMA', 'edwcr_dmx_ac') ,\n",
    "    #     ('${NCR_AC_SCHEMA}', 'edwcr_dmx_ac') ,\n",
    "    #     ('$EDW_PUB_VIEWS', 'edw_pub_views') ,\n",
    "    #     ('!=','<>') , \n",
    "    #     (\"','AS\", \"',' AS\"),\n",
    "    # ]\n",
    "\n",
    "    # post_process_find_replace_list = [\n",
    "    #     {'search':'syslib.length','replace':'length'},\n",
    "    #     {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-ops`.bqutil_fns.isnumeric'},\n",
    "    # ]\n",
    "\n",
    "    # post_process_regex_find_replace_list = [\n",
    "    #     {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "    #     'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "    #     {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-ops`.bqutil_fns.\"}\n",
    "    # ]\n",
    "\n",
    "    # execution_time_find_replace_list = [\n",
    "    # #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    # ]\n",
    "\n",
    "    # production_parms_find_replace_list = [\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_staging.','replace':'{{ params.param_cr_stage_dataset_name }}.'},        \n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr.','replace':'{{ params.param_cr_core_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_base_views.','replace':'{{ params.param_cr_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.bqutil_fns.','replace':'{{ params.param_cr_bqutil_fns_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.auth_base_views.','replace':'{{ params.param_cr_auth_base_views_dataset_name }}.'},\n",
    "    #     {'search':'`hca-hin-dev-cur-ops`.edwcr_views.','replace':'{{ params.param_cr_views_dataset_name }}.'}\n",
    "    # ]\n",
    "\n",
    "    # production_regex_find_replace_list = [\n",
    "    #     {'search':r\"((?:\"+'|'.join(core_dataset_list)+\")\\.[a-z|0-9|_]+\\s*[AS]*\\s*(?!ON)(?!WHERE)(?!INNER)(?!JOIN)[a-z|0-9|_]*)\",\n",
    "    #     'replace':r\"\\1 FOR SYSTEM_TIME AS OF TIMESTAMP(tableload_start_time,'US/Central')\",\n",
    "    #     'query_type':\"EXP\"},\n",
    "    #     {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)\\s*=\\s*current_date\\('US\\/Central'\\)\\s*\", \n",
    "    #     'replace':r\"\\1 >= tableload_start_time - INTERVAL 1 MINUTE \",\n",
    "    #     'query_type':\"ACT\"},\n",
    "    #     {'search':r\"([a-z|0-9|_]+.dw_last_update_date_time)\\s*[<>=]\\s*\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    #     'replace':r\"\\1 = current_date('US/Central') \",\n",
    "    #     'query_type':\"EXP\"},\n",
    "    #     {'search':r\"\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "    #     'replace':r\" tableload_start_time - INTERVAL 1 MINUTE \",\n",
    "    #     'query_type':\"ACT\"}\n",
    "    # ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # IM Replacements\n",
    "\n",
    "    pre_process_find_replace_list = [\n",
    "        ('$NCR_TGT_SCHEMA','edwim'),\n",
    "        ('${NCR_TGT_SCHEMA}','edwim') ,\n",
    "        ('$NCR_STG_SCHEMA','edwim_staging'),\n",
    "        ('${NCR_STG_SCHEMA}','edwim_staging'),\n",
    "        ('$NCR_STG_TGT','edwim_staging')  ,\n",
    "        ('$NCR_BASE_VIEWS','edwim_base_views')  ,\n",
    "        ('$EDWCR_BASE_VIEWS','edwim_base_views')  ,\n",
    "        ('$NCR_TGT','edwim') ,\n",
    "        ('$NCR_AC_VIEW','edwim_dmx_ac_base_views') ,\n",
    "        ('${NCR_AC_VIEW}', 'edwim_dmx_ac_base_views') ,\n",
    "        ('$NCR_AC_SCHEMA', 'edwim_dmx_ac') ,\n",
    "        ('${NCR_AC_SCHEMA}', 'edwim_dmx_ac') ,\n",
    "        ('$EDW_PUB_VIEWS', 'edw_pub_views') ,\n",
    "        ('!=','<>') , \n",
    "        (\"','AS\", \"',' AS\"),\n",
    "    ]\n",
    "\n",
    "        post_process_find_replace_list = [\n",
    "            {'search':'syslib.length','replace':'length'},\n",
    "            {'search':'syslib.isnumeric','replace':'`hca-hin-dev-cur-pub`.bqutil_fns.isnumeric'},\n",
    "    ]\n",
    "\n",
    "    post_process_regex_find_replace_list = [\n",
    "        {'search':r\"bqutil\\.fn\\.cw_td_strtok\\([ ]*([.|a-z|_|0-9]+)[ ]*,[ ]*'(.)'[ ]*,[ ]*([0-9]+)\\)\",\n",
    "        'replace':r\"SPLIT(\\1, '\\2')[ORDINAL(\\3)]\"},\n",
    "        {'search':r\"bqutil\\.fn\\.\", 'replace':r\"`hca-hin-dev-cur-pub`.bqutil_fns.\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.deficiency_audit\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.deficiency_audit\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edw_pub_views\\.clinical_facility\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.clinical_facility\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.encnt_to_role\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.encnt_to_role\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.clinical_health_care_provider\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.clinical_health_care_provider\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwcdm_base_views\\.prctnr_role_idfn\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.prctnr_role_idfn\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.clinical_user_patient_audit\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.clinical_user_patient_audit\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.pk_encounter\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.pk_encounter\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.junc_pk_user_access_level\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.junc_pk_user_access_level\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.ref_pk_data_base_instance\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.ref_pk_data_base_instance\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.document_work_flow_instance\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.document_work_flow_instance\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwim_base_views\\.pk_login_information\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.pk_login_information\"},\n",
    "        {'search':r\"`hca-hin-dev-cur-comp`\\.edwdw_base_views\\.document_work_flow_instance\", 'replace':r\"`hca-hin-dev-cur-comp`.auth_base_views.document_work_flow_instance\"}\n",
    "    ]\n",
    "\n",
    "    execution_time_find_replace_list = [\n",
    "    #    {'search':'`hca-hin-dev-cur-parallon`.edw_pub_views.','replace':'`hca-hin-dev-cur-parallon`.edw_pub_views.'},\n",
    "    ]\n",
    "\n",
    "    production_parms_find_replace_list = [\n",
    "        {'search':'`hca-hin-dev-cur-comp`.edwim_staging.','replace':'{{ params.param_im_stage_dataset_name }}.'},        \n",
    "        {'search':'`hca-hin-dev-cur-comp`.edwim.','replace':'{{ params.param_im_core_dataset_name }}.'},\n",
    "        {'search':'`hca-hin-dev-cur-comp`.edwim_base_views.','replace':'{{ params.param_im_base_views_dataset_name }}.'},\n",
    "        {'search':'`hca-hin-dev-cur-comp`.edwim_views.','replace':'{{ params.param_im_views_dataset_name }}.'},\n",
    "        {'search':'`hca-hin-dev-cur-comp`.auth_base_views.','replace':'{{ params.param_im_auth_base_views_dataset_name }}.'},\n",
    "        {'search':'`hca-hin-dev-cur-pub`.bqutil_fns.','replace':'{{ params.param_im_bqutil_fns_dataset_name }}.'}\n",
    "    ]\n",
    "\n",
    "    production_regex_find_replace_list = [\n",
    "        {'search':r\"((?:\"+'|'.join(core_dataset_list)+\")\\.[a-z|0-9|_]+\\s*[AS]*\\s*(?!ON)(?!WHERE)(?!INNER)(?!JOIN)[a-z|0-9|_]*)\",\n",
    "        'replace':r\"\\1 FOR SYSTEM_TIME AS OF TIMESTAMP(tableload_start_time,'US/Central')\",\n",
    "        'query_type':\"EXP\"},\n",
    "        {'search':r\"DATE\\(([a-z|0-9|_|.]+)\\)\\s*=\\s*current_date\\('US\\/Central'\\)\\s*\", \n",
    "        'replace':r\"\\1 >= tableload_start_time - INTERVAL 1 MINUTE \",\n",
    "        'query_type':\"ACT\"},\n",
    "        {'search':r\"([a-z|0-9|_]+.dw_last_update_date_time)\\s*[<>=]\\s*\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "        'replace':r\"\\1 = current_date('US/Central') \",\n",
    "        'query_type':\"EXP\"},\n",
    "        {'search':r\"\\(\\s*SELECT\\s*MAX\\(etl_job_run\\.job_start_date_time\\).*FROM.*etl_job_run.*WHERE.*=.*'\\s*\\)\", \n",
    "        'replace':r\" tableload_start_time - INTERVAL 1 MINUTE \",\n",
    "        'query_type':\"ACT\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the DDLs for the tables from teradata \n",
    "def gen_bq_validation_sql():\n",
    "\n",
    "    # Download the ParmFiles from required source system folder under the LOB\n",
    "    # sftp_download_bteqs_multiple_sources(unix_server, username, password, unix_etl_folder, unix_lob, parmfiles_path_folder, lob_sub_folders, file_prefix, add_source_dir_prefix, parmfolder_before_lob_subfolder)\n",
    "\n",
    "    # Create Raw SQL Files for BQMS Conversions\n",
    "    control_rec_list = capture_variables_and_gen_td_sql_files(parmfiles_path_folder, conversion_source_path_folder, sub_folder_list)\n",
    "    # print(control_rec_list)\n",
    "\n",
    "    # Preprocess SQL Files to remove undesired code and format SQL\n",
    "    control_rec_list = preprocess_sqls(conversion_source_path_folder, preprocessed_tdsqlfiles_path, sub_folder_list, control_rec_list)\n",
    "    # print(control_rec_list)\n",
    "\n",
    "    ######################################## RUN BQMS HERE ########################################\n",
    "    # Run BQMS Process Here\n",
    "    # bqms_translator(preprocessed_tdsqlfiles_path, conversion_target_path_folder, sub_folder_list, bucket_name, gcsfolder, run_time, 'BTEQ')\n",
    "\n",
    "    # Postprocess SQL Files\n",
    "    # postprocess_bqsqls(conversion_target_path_folder, postprocessed_bqsqlsfiles_path, sub_folder_list)\n",
    "\n",
    "    # Execute Postprocessed SQL FIles\n",
    "    # execute_sql(postprocessed_bqsqlsfiles_path, sub_folder_list)\n",
    "\n",
    "    # Print Execution Results\n",
    "    # print_execution_results(logfilenamepath)\n",
    "\n",
    "    # Parameterize the sqls\n",
    "    # productionize_sqls(postprocessed_bqsqlsfiles_path, prodready_bqsqls_path)\n",
    "\n",
    "    # Create Validation SQLs Files from Template\n",
    "    # gen_validation_sqls_from_template(control_rec_list)\n",
    "\n",
    "    # Upload the Templated SQLs to DAG\n",
    "    upload_to_dags(output_path_folder)\n",
    "    \n",
    "    # Create Control File\n",
    "    gen_control_file(control_rec_list)\n",
    "    \n",
    "    dt2 = datetime.now()\n",
    "    print(dt2-dt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Begin of Processing\")\n",
    "\n",
    "gen_bq_validation_sql()\n",
    "\n",
    "print(\"End of Processing\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
